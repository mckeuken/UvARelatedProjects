{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing pipeline ISLAND MRI examcard\n",
    "\n",
    "**Code version**: 2.0 (Nov. 2021)  \n",
    "**Code by:**  \n",
    "\n",
    "    Max Keuken,\n",
    "    University of Amsterdam, The Netherlands\n",
    "    mckeuken@gmail.com\n",
    "    \n",
    "**Goal of project and code:**  \n",
    "\n",
    "    In collaboration with Mark Hinder, Jane Alty, Jak Ma Wen, Sarah Kemp, Pilou Bazin and Birte Forstmann \n",
    "    we tried to come up with a flexible 3T MRI exam card that could potentially be used as part of the \n",
    "    ISLAND study that is being done in Tasmania Australia. \n",
    "    \n",
    "    The main goal of the exam card is that should provide isotropic, multiparameter, quantitative maps that\n",
    "    can be acquired in a short amount of time. \n",
    "    \n",
    "    The exam card consists of the following sequences:\n",
    "    - mp2rage (which delivers T1w and T1-maps)\n",
    "    - 3DEPI (which delivers SWI and QSM)\n",
    "    - DWI (30 directions which delivers ADC, AD, FA, RD)\n",
    "    - ASL\n",
    "    \n",
    "    Given the nature of the sequences a bit of processing will need to be done to actually get the quantitative \n",
    "    maps that are all in the same space. This is what this collection of notebooks are for.\n",
    "    \n",
    "**Data format**\n",
    "\n",
    "    Each subject's acquired exam card is exported as a DICOM\n",
    "    Each subjects DICOM folder has the following foldername structure:\n",
    "    ISLAND_sub-0001_ses-1 (for the first session of subject 1 within the island study)\n",
    "    ISLAND_pilot-001 (for the first pilot subject within the island study)\n",
    "\n",
    "**Software requirements:**\n",
    "    \n",
    "    Initially the notebook was written on a macbook pro, M1 core (2020) but the QSM processing \n",
    "    needed substantially more ram memory (+/- 20gb). The code is now written for a linux server\n",
    "    where you have the following software installed at the terminal command line level: \n",
    "    dcm2nix v1.0.20190902\n",
    "    mrtrix3 v3.02\n",
    "    fsl v5\n",
    "    ants v2.3.5\n",
    "    python v3.7.7\n",
    "    singularity V3+ (https://sylabs.io)\n",
    "    \n",
    "    Once you have singularity installed, you need to install the images via singularity:\n",
    "    QSM related: https://github.com/CAIsr/qsm\n",
    "\n",
    "    It needs to work via the terminal command line, which can be tested by typing in\n",
    "    tgv_qsm \n",
    "    \n",
    "    I would also recommend that you install the jupyter lab extension '@jupyterlab/toc' as this\n",
    "    allows you to go through the code much easier using the table of content tab at the left side\n",
    "    of your jupyterlab interface\n",
    "        \n",
    "**MNI Atlas requirement**\n",
    "    \n",
    "    Basically any atlas that is in standard space can be used pretty easily in this pipeline. The\n",
    "    only thing that you have to make sure is that your atlas has the following things:\n",
    "    - it needs to have the exact same dimensions as the MNI template used in this pipeline. Even \n",
    "         when it is a MNI based atlas the dimension of the file can still be different. This can\n",
    "         be solved by running a fix registration step using FSL FLIRT. The actual code to do so is \n",
    "         given in the section where the MNI atlas is used.\n",
    "    - it needs to have a text file that per row has the label of the structure \n",
    "    - it needs the actual atlas which should be an nifti file where every single integer value\n",
    "        corresponds to a mask. Also note that the order of anatomical labels in the text file \n",
    "        and the nifti file need to be IDENTICAL. \n",
    "    \n",
    "**How to run this notebook?**\n",
    "\n",
    "    If you have never worked with jupyter notebooks / jupyter labs, have a look at https://ipython-books.github.io/\n",
    "    tldr: select a cell that you want to run and press 'shift+enter' on a mac/linux and you will run the block of code\n",
    "    \n",
    "**Status of code**\n",
    "\n",
    "- [x] modules\n",
    "- [x] paths\n",
    "- [x] external files\n",
    "- [x] dicom 2 nifti\n",
    "- [x] mp2rage\n",
    "- [x] 3DEPI\n",
    "- [x] DWI\n",
    "- [x] ASL\n",
    "- [x] registration to mp2rage space\n",
    "- [X] registration to mni space\n",
    "- [X] extract the qMRI values per atlas region and export to excel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Setup the current enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the required python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Python modules\n",
    "#################################################################################\n",
    "#. Note that if you are missing one of the following packages you can install \n",
    "#. it via 'pip install 'package name'' on the command line\n",
    "\n",
    "####\n",
    "# standard python modules\n",
    "###\n",
    "import shutil, sys, os, re, subprocess, json, numpy as np, pandas as pd, glob as glob, warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "#. If you are debugging you might want to turn this back on again\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "####\n",
    "# MRI related python modules\n",
    "####\n",
    "#. We are using the pymp2rage python package which is programmed by Gilles de Hollander\n",
    "#. and is based on the paper by Jose Marques et al. (2010) neuroimage. \n",
    "#. \n",
    "#. You can install this package by typing pip install git+https://github.com/mckeuken/pymp2rage\n",
    "#. The original package is found here: https://github.com/Gilles86/pymp2rage/ but contains a typo\n",
    "#. in the setup file which interferes with the installation. Hence my own fork.\n",
    "#. See https://github.com/Gilles86/pymp2rage/issues/3 for the description of the typo. \n",
    "import pymp2rage, nibabel as nib, bids as bids\n",
    "import nipype.interfaces.fsl as fsl\n",
    "from nipype.interfaces.ants import RegistrationSynQuick\n",
    "from nipype.interfaces.ants import ApplyTransforms\n",
    "import ants\n",
    "\n",
    "####\n",
    "# Visualisation related python modules\n",
    "####\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from nilearn import plotting\n",
    "%matplotlib inline\n",
    "\n",
    "print('All modules have been imported')\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the paths and determining the subject names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Path settings and subject names\n",
    "#################################################################################\n",
    "#. In the end we want to have a folder that is BIDS compatible as this will\n",
    "#. make your life a lot easier when dealing with published pipelines and it \n",
    "#. also give you a good folder structure to work with. In general we want \n",
    "#. to have a layout where we have the following:\n",
    "#. - subject\n",
    "#. -- session <- necessary because the island study is longitudinal\n",
    "#. --- anatomy folder that contains the t1 and t2* related data\n",
    "#. --- diffusion weighted data folder\n",
    "#. --- perfusion weighted data folder\n",
    "#. --- dicom 2 nifti folder that contains all the raw exported scans\n",
    "\n",
    "####\n",
    "# Set main project folder \n",
    "#. NOTE: These folders need to exist already \n",
    "projectDir = '/home/mkeuken1/Documents/Projects/TasScans' # <- !!!CHANGE BASED ON OWN SERVER!!!\n",
    "syntaxDir = projectDir+'/syntax'\n",
    "rawDataDir = projectDir+'/original/dicom' # plus a session folder: 'pilot', 'ses-0001', 'ses-0002', etc\n",
    "processedDataDir = projectDir+'/data'\n",
    "\n",
    "# Location of the MNI template and the atlas used\n",
    "#. This is the standard 1mm MNI template as is incorporated within FSL\n",
    "mniTemplate =  syntaxDir+'/avg152T1_brain.nii.gz'       \n",
    "mniAtlasLabels = syntaxDir+'/MNIAtlas/BN_Atlas_246_LUT.txt'\n",
    "mniAtlas = syntaxDir+'/MNIAtlas/BN_Atlas_246_1mm.nii'\n",
    "\n",
    "####\n",
    "#. Which acquisition session are we processing?\n",
    "session = 'pilot' # Options: 'pilot', 'ses-0001', 'ses-0002', etc\n",
    "\n",
    "####\n",
    "# Get all the subject IDs from the dicom folders\n",
    "# Step 1) list all the subject folders and transform them to lowercase\n",
    "subjects_dicom = glob.glob(rawDataDir+'/'+session+'/*')\n",
    "subjects_Tmp = [item.lower() for item in subjects_dicom]\n",
    "\n",
    "# Step 2) Rename the subject lists to match the BIDS format:\n",
    "subjects = []\n",
    "for i in range(len(subjects_Tmp)):\n",
    "    # Split the path into the different folders\n",
    "    foldername = subjects_Tmp[i].split('/') \n",
    "    # Get the numbers as written down in the subject (last) folder:\n",
    "    numericsInFoldername = re.findall(r'\\d+', foldername[-1])[0]\n",
    "    # Add the number to the string 'sub' while padding the number of zero's\n",
    "    #. note that it goes upto 9999 subjects. If you need a wider range change \n",
    "    #. the 4 to say 5 (and thus 99999) subjects\n",
    "    bidsName ='sub-'+str(numericsInFoldername).zfill(4)\n",
    "    subjects.append(bidsName)\n",
    "# Lets also sort the subjects so that it is a bit nicer\n",
    "subjects_dicom.sort()\n",
    "subjects.sort()\n",
    "# Print the number of subjects that we now have\n",
    "print('The number of subjects for which we have MRI data of this session is: ', len(subjects))\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Custom config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Custom config files\n",
    "#################################################################################\n",
    "#. For the DWI we are acquiring extra files so that we can apply topup. For that\n",
    "#. you need a topup configuration file that is part of fsl or you can specify \n",
    "#. one yourselve which we do in our case. \n",
    "\n",
    "# This config file is an ALTERED VERSION OF FSL's b02b0.cnf and can be used for higher resolution data.\n",
    "# The file itself has the following information:\n",
    "customb02b0 = \"\"\"# Resolution (knot-spacing) of warps in mm\n",
    "--warpres=20,16,14,12,10,6,4,4,4,2.5\n",
    "# Subsampling level (a value of 2 indicates that a 2x2x2 neighbourhood is collapsed to 1 voxel)\n",
    "--subsamp=2,2,2,2,2,1,1,1,1,1\n",
    "# FWHM of gaussian smoothing\n",
    "--fwhm=8,6,4,3,3,2,1,0,0,0\n",
    "# Maximum number of iterations\n",
    "--miter=5,5,5,5,5,10,10,20,20,25\n",
    "# Relative weight of regularisation\n",
    "--lambda=0.005,0.001,0.0001,0.000015,0.000005,0.0000005,0.00000005,0.0000000005,0.00000000001,0.00000000001\n",
    "# If set to 1 lambda is multiplied by the current average squared difference\n",
    "--ssqlambda=1\n",
    "# Regularisation model\n",
    "--regmod=bending_energy\n",
    "# If set to 1 movements are estimated along with the field\n",
    "--estmov=1,1,1,1,1,0,0,0,0,0\n",
    "# 0=Levenberg-Marquardt, 1=Scaled Conjugate Gradient\n",
    "--minmet=0,0,0,0,0,1,1,1,1,1\n",
    "# Quadratic or cubic splines\n",
    "--splineorder=3\n",
    "# Precision for calculation and storage of Hessian\n",
    "--numprec=double\n",
    "# Linear or spline interpolation\n",
    "--interp=spline\n",
    "# If set to 1 the images are individually scaled to a common mean intensity \n",
    "--scale=1\"\"\"\n",
    "\n",
    "###\n",
    "# Create the b02b0_hires.cnf file:\n",
    "with open(syntaxDir+'/b02b0_hires.cnf', 'w+') as f:\n",
    "    f.writelines(customb02b0)\n",
    "    \n",
    "print('Custom b02b0 file has been created')\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the NIfTI files from the raw dicoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## 1. Convert raw data to NIfTI using dcm2niix\n",
    "#################################################################################\n",
    "####\n",
    "#. If you want to rerun the code (and thus OVERWRITE your previous results),\n",
    "#. set overwrite to True\n",
    "overwrite = True\n",
    "####\n",
    "\n",
    "####\n",
    "# Start the loop to export the dicom data to nifti\n",
    "####\n",
    "#. Lets start with an empty list for summary info:\n",
    "summaryScans = []\n",
    "\n",
    "#. after which we can start the for loop to create a folder per subject\n",
    "for i in range(len(subjects)):\n",
    "    print('\\n', subjects[i])\n",
    "    #. BIDS'ish layout: subject, session, modality, data\n",
    "    #. Where we make a seperate folder for pilot data and final data in the ISLAND folder\n",
    "    if session == 'pilot':\n",
    "        work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "    else:\n",
    "        work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "    \n",
    "    dicom_dir = subjects_dicom[i]\n",
    "    convert_dir = work_dir+'/dcm2nii' # This will contain all the exported NIfTI files from all scans\n",
    "    anat_dir = work_dir+'/anat' # which will contain the T1 and T2* derived images\n",
    "    dwi_dir = work_dir+'/dwi' # this will contain all the DWI derived images\n",
    "    \n",
    "    # Create folder and convert dicom to nifti\n",
    "    if ((os.path.exists(convert_dir)) & (overwrite == False)):\n",
    "        print('Conversion to NIfTI: done (set overwrite to True to recompute)')\n",
    "    else:\n",
    "        print('\\n Converting Dicom into NIfTI files')\n",
    "        try: \n",
    "            shutil.rmtree(convert_dir)\n",
    "            os.makedirs(convert_dir)\n",
    "        except:\n",
    "            os.makedirs(convert_dir)\n",
    "        command = \"dcm2niix -o \"+convert_dir+' -z y '+dicom_dir\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell = True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        #. As you can see in the folder output, the exam card has quite a lot of extra scans that we \n",
    "        #. didn't really acquire. Instead the same data is exported multiple times and includes\n",
    "        #. other files that were derived from the scanner console. We are going to\n",
    "        #. clean up the output folder by making a folder of scansOfNoInterest which will include the\n",
    "        #. data that is exported twice.\n",
    "        #. First make the folder\n",
    "        scansOfNoInterest_dir = convert_dir+'/scansOfNoInterest'\n",
    "        if not os.path.exists(scansOfNoInterest_dir):\n",
    "            os.makedirs(scansOfNoInterest_dir)\n",
    "        else:\n",
    "            shutil.rmtree(scansOfNoInterest_dir)          \n",
    "            os.makedirs(scansOfNoInterest_dir)\n",
    "\n",
    "        #. Which files do we want to move?\n",
    "        #. Any files that have a file counter in there that is larger than 99\n",
    "        scansThatWeWantToMove = glob.glob(convert_dir+'/*.*' , recursive=False)\n",
    "        for scan in scansThatWeWantToMove:\n",
    "            fileName = scan.split('/')\n",
    "            examCardCounter = re.findall(r'\\d+', fileName[-1])[-1]\n",
    "            if int(examCardCounter) > 99:\n",
    "                 shutil.move(scan, scansOfNoInterest_dir)\n",
    "\n",
    "        # We also want to move the files that were derived at the scanner console\n",
    "        #. We can find this information in the json file, field 'ImageType'\n",
    "        # First make the folder:\n",
    "        consoleDerived_dir = convert_dir+'/consoleDerived'\n",
    "        if not os.path.exists(consoleDerived_dir):\n",
    "            os.makedirs(consoleDerived_dir)\n",
    "        else:\n",
    "            shutil.rmtree(consoleDerived_dir)          \n",
    "            os.makedirs(consoleDerived_dir)\n",
    "            \n",
    "        #. Which files do we want to move?\n",
    "        #. Find the json files that we still have in the main folder\n",
    "        scansThatWeWantToMove = glob.glob(convert_dir+'/*.json' , recursive=False)\n",
    "        #. Open the actual json file and check whether the string 'DERIVED'\n",
    "        #. occures in the field 'ImageType'. Ifso, then move all files with\n",
    "        #. all the different extensions to the console derived folder\n",
    "        for jsonFile in scansThatWeWantToMove:\n",
    "            tmpJson = open(jsonFile)\n",
    "            JsonData = json.load(tmpJson)\n",
    "            if 'DERIVED' in JsonData['ImageType']:\n",
    "                moveFiles = glob.glob(jsonFile.rsplit( \".\", 1 )[ 0 ]+'.*' , recursive=False)\n",
    "                for file in moveFiles:\n",
    "                    shutil.move(file, consoleDerived_dir)\n",
    "            \n",
    "    #. Get some basic information from the different files\n",
    "    tmpScans = glob.glob(convert_dir+'/*.nii.gz' , recursive=False)\n",
    "\n",
    "    #. For the given scans \n",
    "    for scan in tmpScans:\n",
    "        #. Load the image\n",
    "        dwi_img = nib.load(scan)\n",
    "        #. Get the actual data\n",
    "        dwi_img_data = dwi_img.get_fdata()\n",
    "\n",
    "        #. Get some summary stats on the image and combine them in the list:\n",
    "        dwi_img_shape = dwi_img_data.shape\n",
    "        dwi_img_voxelSize = nib.affines.voxel_sizes(dwi_img.affine)\n",
    "        summaryScans.append([subjects[i], scan.rpartition('/')[-1], dwi_img_shape, np.round(dwi_img_voxelSize, 2)])\n",
    "\n",
    "#. Format the summarylist into a dataframe: \n",
    "dfSummaryScans = pd.DataFrame(summaryScans, columns=['Subject', 'Scan', 'Shape', 'VoxelSize'])\n",
    "dfSummaryScans = dfSummaryScans.sort_values(by = ['Subject', 'Scan']).reset_index(drop = True)\n",
    "\n",
    "#. Based on the exam card we expect to have mp2rage (4x) related files, 3DEPI (2x) related files \n",
    "#. and DWI related files\n",
    "display(dfSummaryScans)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create brain masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MP2RAGE scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#. Skull strip the mp2rage INV2 image using BET and only keep the mask\n",
    "#################################################################################\n",
    "# This part is iteratively done as betting brains is not trival and needs \n",
    "#. quite some custom work. The benefit is that the better you do it now, the \n",
    "#. easier (and probably better) the registration between scans and mni space\n",
    "#. is going to be later on.\n",
    "\n",
    "# Also note that you will need to do this EVERY session again. The code assumes\n",
    "#. that you will run your analysis per session.\n",
    "\n",
    "overwrite = False\n",
    "# List of subjects for which betting is done:\n",
    "#. (this list will have to be made empty again once you start analyzing a \n",
    "#. new session)\n",
    "subjectsDone = [] # For example if we have two subject that we are happy with we add them to the list like this: ['sub-0007', 'sub-0013']\n",
    "\n",
    "# We don't want to have any plotting warnings that have nothing to do with \n",
    "#. the actual betting:\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    # Start the betting per subject\n",
    "    for i in range(0, len(subjects)):\n",
    "        print('\\n', subjects[i])\n",
    "        # select the right folder\n",
    "        if session == 'pilot':\n",
    "            work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "        else:\n",
    "            work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "        convert_dir = work_dir+'/dcm2nii'\n",
    "        bet_dir = work_dir+'/bet'\n",
    "        \n",
    "        # Check if this subject is already done or not (if not create folder)\n",
    "        if ((os.path.exists(bet_dir)) & (overwrite == False) & (subjects[i] in subjectsDone)):\n",
    "            print('BET subject: done (set overwrite to true to recompute AND remove from list subjectsDone )')\n",
    "        else:\n",
    "            print('\\n BET subject')\n",
    "            try: \n",
    "                shutil.rmtree(bet_dir)\n",
    "                os.makedirs(bet_dir)\n",
    "            except:\n",
    "                os.makedirs(bet_dir)\n",
    "\n",
    "            # Location of magnitude INV2 file\n",
    "            anat_t1_related_files = glob.glob(convert_dir+'/*.json' , recursive=False)\n",
    "            for jsonFile in anat_t1_related_files:\n",
    "                fileName = jsonFile.split('.')[0]\n",
    "                tmpJson = open(jsonFile)\n",
    "                jsonData = json.load(tmpJson, strict = False)\n",
    "                if 'INV2' in jsonData['SeriesDescription']:\n",
    "                    if (('NORM' in jsonData['ImageType']) | ('M' in jsonData['ImageType'])):\n",
    "                        inv2_mag = fileName+'.nii.gz'\n",
    "\n",
    "            # Bet the magnitude image using BET and multiple iterations:\n",
    "            # f: fractional intensity threshold (0->1); default=0.5; smaller values give larger brain outline estimates\n",
    "            # g: vertical gradient in fractional intensity threshold (-1->1); default=0; \n",
    "            #.    positive values give larger brain outline at bottom, smaller at top\n",
    "\n",
    "            #. The input files for the betting procedure\n",
    "            in_file = inv2_mag\n",
    "            out_file = bet_dir+'/'+subjects[i]+'_'+session+'_brain_T1w-INV2.nii.gz'\n",
    "            out_file_mask = bet_dir+'/'+subjects[i]+'_'+session+'_brain_T1w-INV2_mask.nii.gz'\n",
    "            \n",
    "            ####\n",
    "            # The betting command that you will need to tweak:\n",
    "            ####\n",
    "            command = 'bet '+in_file+' '+out_file+' -f 0.62 -g 0.15 -m -R'\n",
    "            try:\n",
    "                subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "                raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "            # Plot the data to see if the betting worked\n",
    "            #. If the mask did seem to capture the entire brain and did not remove too much \n",
    "            #. from the brainstem, cerebellum, temporal poles etc add that subject's id to \n",
    "            #. the list 'subjectsDone' above in the following manner: \n",
    "            #.    subjectsDone = ['sub-0001', 'sub-0002'] etc\n",
    "            fig = plt.figure(figsize=(20, 6))\n",
    "            plotting.plot_roi(out_file_mask, \n",
    "                              bg_img=in_file, \n",
    "                              figure=fig,\n",
    "                              cmap = 'Reds',\n",
    "                              alpha = 0.3,\n",
    "                              cut_coords=(0, 0, 0), \n",
    "                              draw_cross= False,\n",
    "                              title = subjects[i] )\n",
    "            \n",
    "            # Clean the bet folder so that we only keep the mask:\n",
    "            os.remove(bet_dir+'/'+subjects[i]+'_'+session+'_brain_T1w-INV2.nii.gz')\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3DEPI scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#. Skull strip the 3D EPI magnitude image using BET and only keep the mask\n",
    "#################################################################################\n",
    "# This part is iteratively done as betting brains is not trival and needs \n",
    "#. quite some custom work. The benefit is that the better you do it now, the \n",
    "#. easier (and probably better) the registration between scans and mni space\n",
    "#. is going to be later on. \n",
    "\n",
    "# Also note that you will need to do this EVERY session again. The code assumes\n",
    "#. that you will run your analysis per session\n",
    "\n",
    "# List of subjects for which betting is done:\n",
    "#. (this list will have to be made empty again once you start analyzing a \n",
    "#. new session)\n",
    "subjectsDone = [] # For example if we have two subject that we are happy with we add them to the list like this: ['sub-0007', 'sub-0013']\n",
    "\n",
    "# We don't want to have any plotting warnings that have nothing to do with \n",
    "#. the actual betting:\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    # Start the betting per subject\n",
    "    for i in range(0, len(subjects)):\n",
    "        print('\\n', subjects[i])\n",
    "        # select the right folder\n",
    "        if session == 'pilot':\n",
    "            work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "        else:\n",
    "            work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "        convert_dir = work_dir+'/dcm2nii'\n",
    "        bet_dir = work_dir+'/bet'\n",
    "        \n",
    "        # Check if this subject is already done or not (if not create folder)\n",
    "        if ( subjects[i] in subjectsDone):\n",
    "            print('BET subject: done (set overwrite to true to recompute AND remove from list subjectsDone )')\n",
    "        else:\n",
    "            print('\\n BET subject')\n",
    "            # Location of magnitude, phase and json files\n",
    "            anat_qsm_related_files = glob.glob(convert_dir+'/*.json' , recursive=False)\n",
    "            for jsonFile in anat_qsm_related_files:\n",
    "                fileName = jsonFile.split('.')[0]\n",
    "                tmpJson = open(jsonFile)\n",
    "                jsonData = json.load(tmpJson, strict=False)\n",
    "                if '3DEPI' in jsonData['SeriesDescription']:\n",
    "                    if (('NORM' in jsonData['ImageType']) | ('M' in jsonData['ImageType'])):\n",
    "                        chimap_mag = fileName+'.nii.gz'\n",
    "        \n",
    "            # Bet the magnitude image using BET and multiple iterations:\n",
    "            # f: fractional intensity threshold (0->1); default=0.5; smaller values give larger brain outline estimates\n",
    "            # g: vertical gradient in fractional intensity threshold (-1->1); default=0; \n",
    "            #.    positive values give larger brain outline at bottom, smaller at top\n",
    "\n",
    "            #. The input files for the betting procedure\n",
    "            in_file = chimap_mag\n",
    "            out_file = bet_dir+'/'+subjects[i]+'_'+session+'_brain_Chimap.nii.gz'\n",
    "            out_file_mask = bet_dir+'/'+subjects[i]+'_'+session+'_brain_Chimap_mask.nii.gz'\n",
    "            \n",
    "            ####\n",
    "            # The betting command that you will need to tweak:\n",
    "            ####\n",
    "            command = 'bet '+in_file+' '+out_file+' -f 0.4 -g 0.2 -m -R'\n",
    "            try:\n",
    "                subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "                raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "            # Plot the data to see if the betting worked\n",
    "            #. If the mask did seem to capture the entire brain and did not remove too much \n",
    "            #. from the brainstem, cerebellum, temporal poles etc add that subject's id to \n",
    "            #. the list 'subjectsDone' above in the following manner: \n",
    "            #.    subjectsDone = ['sub-0001', 'sub-0002'] etc\n",
    "            fig = plt.figure(figsize=(20, 6))\n",
    "            plotting.plot_roi(out_file_mask, \n",
    "                              bg_img=in_file, \n",
    "                              figure=fig,\n",
    "                              cmap = 'Reds',\n",
    "                              alpha = 0.3,\n",
    "                              cut_coords=(0, 0, 0), \n",
    "                              draw_cross= False,\n",
    "                              title = subjects[i] )\n",
    "            \n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the MP2RAGE scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Create T1w, qT1 map\n",
    "#################################################################################\n",
    "#. This section creates the unified T1-weighted images and estimates the \n",
    "#. quantitative T1 maps. Finally we also create a brain mask based on the \n",
    "#. second inversion magnitude image of the mp2rage sequence. \n",
    "#. \n",
    "#. NOTE: To be able to do all this the magnitude AND phase information is required!\n",
    "#. Make sure that this data is recorded and exported in the exam card. \n",
    "\n",
    "# If you want to overwrite your previous results set overwrite to True. To protect\n",
    "#. your data that you have calculated before, set overwrite to False\n",
    "overwrite = False\n",
    "\n",
    "# Create the model fitter with the specific MRI acquistion parameters per subject\n",
    "for i in range(0, len(subjects)):\n",
    "    print('\\n', subjects[i])\n",
    "    if session == 'pilot':\n",
    "        work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "    else:\n",
    "        work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "        \n",
    "    convert_dir = work_dir+'/dcm2nii'\n",
    "    anat_dir = work_dir+'/anat'\n",
    "    \n",
    "    if ((os.path.exists(anat_dir)) & (overwrite == False)):\n",
    "        print('Create T1 unified and T1 map: done (set overwrite to true to recompute)')\n",
    "    else:\n",
    "        print('\\n Create T1 unified and T1 map')\n",
    "        try: \n",
    "            shutil.rmtree(anat_dir)\n",
    "            os.makedirs(anat_dir)\n",
    "        except:\n",
    "            os.makedirs(anat_dir)\n",
    "        \n",
    "        # Location of magnitude, phase and json files\n",
    "        anat_t1_related_files = glob.glob(convert_dir+'/*.json' , recursive=False)\n",
    "        \n",
    "        for jsonFile in anat_t1_related_files:\n",
    "            fileName = jsonFile.split('.')[0]\n",
    "            tmpJson = open(jsonFile)\n",
    "            jsonData = json.load(tmpJson, strict = False)\n",
    "            if 'INV1' in jsonData['SeriesDescription']:\n",
    "                if (('NORM' in jsonData['ImageType']) | ('M' in jsonData['ImageType'])):\n",
    "                    inv1_mag = fileName+'.nii.gz'\n",
    "                    inv1_json = jsonData\n",
    "                elif (('PHASE' in jsonData['ImageType']) | ('P' in jsonData['ImageType'])):\n",
    "                    inv1_ph = fileName+'.nii.gz'\n",
    "            elif 'INV2' in jsonData['SeriesDescription']:\n",
    "                if (('NORM' in jsonData['ImageType']) | ('M' in jsonData['ImageType'])):\n",
    "                    inv2_mag = fileName+'.nii.gz'\n",
    "                    inv2_json = jsonData\n",
    "                elif (('PHASE' in jsonData['ImageType']) | ('P' in jsonData['ImageType'])):\n",
    "                    inv2_ph = fileName+'.nii.gz'\n",
    "\n",
    "        # Define the mp2rage fitter with the acquisiton parameters from the json file and the number of z slices\n",
    "        zImage = nib.load(inv1_mag)\n",
    "        #. Get the data and dimensions of the mp2rage scan:\n",
    "        zImage = zImage.get_fdata()\n",
    "        zImage_shape = zImage.shape\n",
    "        \n",
    "        # Fit the pymp2rage fitter to the actual data\n",
    "        #. Note that you might get a warning that states something along the lines of 'True_divide'. \n",
    "        #. This is fine and can be ignored.\n",
    "        fitter = pymp2rage.MP2RAGE(MPRAGE_tr = inv1_json['RepetitionTime'],\n",
    "                                   invtimesAB = [inv1_json['InversionTime'], inv2_json['InversionTime']],\n",
    "                                   flipangleABdegree = [inv1_json['FlipAngle'], inv2_json['FlipAngle']],\n",
    "                                   nZslices = zImage_shape[-1],\n",
    "                                   FLASH_tr = [inv1_json['EchoTime'], inv2_json['EchoTime']],\n",
    "                                   B0 = inv1_json['MagneticFieldStrength'],\n",
    "                                   inv1 = inv1_mag, \n",
    "                                   inv1ph = inv1_ph,\n",
    "                                   inv2 = inv2_mag,                      \n",
    "                                   inv2ph = inv2_ph)\n",
    "\n",
    "        # Estimate the Unified T1 weighted image and plot it:\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        plotting.plot_anat(fitter.t1w_uni, figure=fig, cut_coords=(0, 0, 0), title = subjects[i] )\n",
    "\n",
    "        # As the T1 map is quantitative, the values actually mean something. Therefore you can \n",
    "        #. add some color and some max values\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        plotting.plot_stat_map(fitter.t1map, cmap=plt.cm.viridis, figure=fig, symmetric_cbar=False, vmax=4000, title = subjects[i])\n",
    "\n",
    "        # Export the T1 map and T1 unified to the local folder as nifti's\n",
    "        fitter.t1map.to_filename(anat_dir+'/'+subjects[i]+'_'+session+'_T1map.nii.gz')\n",
    "        fitter.t1w_uni.to_filename(anat_dir+'/'+subjects[i]+'_'+session+'_T1w.nii.gz')\n",
    "        \n",
    "        # Also save a json file to the same folder which contains the main T1w parameters as \n",
    "        #. based from the INV1 mag file\n",
    "        with open(anat_dir+'/'+subjects[i]+'_'+session+'_T1w.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(inv1_json, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the 3D EPI scans into QSM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Process the 3D EPI data\n",
    "#################################################################################\n",
    "overwrite = True\n",
    "\n",
    "# run the TGV_QSM pipeline on the 3DEPI data\n",
    "for i in range(0, len(subjects)):\n",
    "    print('\\n', subjects[i])\n",
    "    if session == 'pilot':\n",
    "        work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "    else:\n",
    "        work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "        \n",
    "    convert_dir = work_dir+'/dcm2nii'\n",
    "    anat_dir = work_dir+'/anat'\n",
    "    bet_dir = work_dir+'/bet'\n",
    "    # As the QSM data will end up in the same folder as the T1w data we cannot\n",
    "    #. check if the folder just exists and thus remove it etc. If we want to rerun \n",
    "    #. the QSM part as that would also remove the T1 processed data. We therefore \n",
    "    #. need to check if a certain file exists:\n",
    "    qsmFileExists = anat_dir+'/'+subjects[i]+'_'+session+'_Chimap.nii.gz' # According to the BIDS format: QSM -> Chimap\n",
    "    \n",
    "    if ((os.path.exists(dwi_dir+'/')) & (overwrite == False)):\n",
    "        print('Process the 3D EPI data: done (set overwrite to true to recompute)')\n",
    "    else:\n",
    "        print('\\n Process the 3D EPI data')\n",
    "        \n",
    "        # Location of magnitude, phase and json files\n",
    "        anat_qsm_related_files = glob.glob(convert_dir+'/*.json' , recursive=False)\n",
    "        for jsonFile in anat_qsm_related_files:\n",
    "            fileName = jsonFile.split('.')[0]\n",
    "            tmpJson = open(jsonFile)\n",
    "            jsonData = json.load(tmpJson, strict=False)\n",
    "            if '3DEPI' in jsonData['SeriesDescription']:\n",
    "                if (('NORM' in jsonData['ImageType']) | ('M' in jsonData['ImageType'])):\n",
    "                    chimap_mag = fileName+'.nii.gz'\n",
    "                    chimap_json = jsonData\n",
    "                elif (('PHASE' in jsonData['ImageType']) | ('P' in jsonData['ImageType'])):\n",
    "                    chimap_ph = fileName+'.nii.gz'\n",
    "        \n",
    "        # Assign the mask that is based on the magnitude to a variable:\n",
    "        #. This file was created using the betting step in the beginning of the notebook.\n",
    "        chimap_mag_mask = bet_dir+'/'+subjects[i]+'_'+session+'_brain_Chimap_mask.nii.gz'\n",
    "        \n",
    "        # Run the QSM fitting (This might take a while and take up consideral RAM while doing so)\n",
    "        #. where we provide the different parameters directly from the json file. Note that it \n",
    "        #. will automatically use all available cores if OpenMP multithreading is available. \n",
    "        #. This is a good thing but might interfer with other users on the same server..\n",
    "        command = 'tgv_qsm -p '+chimap_ph+' -m '+chimap_mag_mask+' -f '+str(float(chimap_json['MagneticFieldStrength']))+' -t '+str(chimap_json['EchoTime'])+' -s -o _qsm_' \n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell = True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "         \n",
    "        # tgv_qsm outputs the qsm file in the same folder as our input data which is still located in the \n",
    "        #. dcm2nii folder. So lets rename to BIDS format and move the output file to the anat folder\n",
    "        old_name = glob.glob(convert_dir+'/*qsm*', recursive=False)[0]  \n",
    "        new_name =  anat_dir+'/'+subjects[i]+'_'+session+'_Chimap.nii.gz'\n",
    "        os.rename(old_name, new_name)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the DWI scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Process the DWI data\n",
    "#################################################################################\n",
    "# The DWI data will be processed in a way to ensure that it is corrected for noise,\n",
    "#. eddy current, topup, bias field etc after which a diffusion tensor model \n",
    "#. is fit to the cleaned up data. The resulting DTI data is used to extract a number\n",
    "#. of metrics such as FA, RD, MD, and ADC. \n",
    "overwrite = False\n",
    "saveIntermediatedFiles = True # Note I would always keep the intermediate files\n",
    "\n",
    "# Process the DWI data\n",
    "for i in range(0, len(subjects)):\n",
    "    print('\\n', subjects[i])\n",
    "    \n",
    "    # Determine the main folders:\n",
    "    if session == 'pilot':\n",
    "        work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "    else:\n",
    "        work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "        \n",
    "    convert_dir = work_dir+'/dcm2nii'\n",
    "    anat_dir = work_dir+'/anat'\n",
    "    dwi_dir = work_dir+'/dwi'\n",
    "    dwiInterim_dir = dwi_dir+'/intermediateFiles'\n",
    "    # Check if the DWI folder already exists and whether you want to remove it\n",
    "    #. and rerun it again.\n",
    "    if ((os.path.exists(dwi_dir)) & (overwrite == False)):\n",
    "        print('Process the DWI data: done (set overwrite to true to recompute)')\n",
    "    else:\n",
    "        print('\\n Process the DWI data')\n",
    "        try: \n",
    "            shutil.rmtree(dwiInterim_dir, ignore_errors=True)\n",
    "            shutil.rmtree(dwi_dir, ignore_errors=True)\n",
    "            os.makedirs(dwi_dir)\n",
    "        except:\n",
    "            os.makedirs(dwi_dir)\n",
    "            \n",
    "        # We are also going to make a tmp folder where we are going to save all the\n",
    "        #. preprocessing steps. You can automatically remove those at the end by \n",
    "        #. setting saveIntermediatedFiles to False at the top of this module. Given \n",
    "        #. the number of files and filesize you probably want to remove them in the end\n",
    "        #. but if the output doesnt make any sense I would keep the files so that you \n",
    "        #. can check where it goes haywire.\n",
    "        os.makedirs(dwiInterim_dir)\n",
    "\n",
    "        ##############################\n",
    "        # 1. Convert to MRtrix3 format\n",
    "        print('\\n1. Convert Nifti files into Mrtrix3 files')\n",
    "        ##############################\n",
    "        # Create a list of different file names based on the DWI files that we have\n",
    "        #. Note that these are not the B0 scans that we are going to use for topup\n",
    "        #. (as they don't have a corresponding bvals and bvecs file)\n",
    "        bval_files = glob.glob(convert_dir+'/*.bval')\n",
    "        for bval_file in bval_files:\n",
    "            bvec_file = bval_file.replace('.bval','.bvec')\n",
    "            data_file = bval_file.replace('.bval','.nii.gz')\n",
    "            mif_file = bval_file.replace('.bval','.mif.gz')\n",
    "            mif_file = mif_file.replace(convert_dir, dwiInterim_dir)\n",
    "\n",
    "            command = 'mrconvert -fslgrad '+bvec_file+' '+bval_file+' '+data_file+' '+mif_file\n",
    "            try:\n",
    "                subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "                raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Convert the topup related files:\n",
    "        topup_files = glob.glob(convert_dir+'/*_phase_*.nii.gz')\n",
    "        for topup_file in topup_files:\n",
    "            data_file = topup_file\n",
    "            bval_file = 'dummy_topup.bval'\n",
    "            bvec_file = 'dummy_topup.bvec'\n",
    "            mif_file = topup_file.replace('.nii.gz','.mif.gz')\n",
    "            mif_file = mif_file.replace(convert_dir, dwiInterim_dir)\n",
    "\n",
    "            command = 'mrconvert '+data_file+' '+mif_file\n",
    "            try:\n",
    "                subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "                raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        ##############################         \n",
    "        # 2. Concatenate the different DWI files\n",
    "        print('\\n2. Concatenate the DWI files')\n",
    "        ##############################\n",
    "        mif_files = glob.glob(dwiInterim_dir+'/*.mif.gz')\n",
    "\n",
    "        # Empty list of DWI related .mif.gz files\n",
    "        dwi_files = []\n",
    "        topup_files = []\n",
    "        for mif_file in mif_files:\n",
    "            if mif_file.find('_phase_')>-1:\n",
    "                topup_files.append(mif_file)\n",
    "            else:\n",
    "                dwi_files.append(mif_file)\n",
    "        # We need to sort them so that the first volume has the same phase encoding\n",
    "        #. als the main DWI file (important for TOPUP step); \n",
    "        #. We are ASSUMING that the main DWI file is AP and the extra phase encoding is PA\n",
    "        topup_files.sort()\n",
    "\n",
    "        # Merge the DWI files\n",
    "        if len(dwi_files) <= 1:\n",
    "            print('\\nThere is only a single DWI file so no need to merge')\n",
    "            shutil.copy(dwi_files[0], dwiInterim_dir+'/dti_raw.mif.gz')\n",
    "        else:\n",
    "            print('\\nmerge '+str(len(dwi_files))+' dwi files')\n",
    "            command = 'mrcat -axis 3 '\n",
    "            for dwi_file in dwi_files:\n",
    "                command = command+dwi_file+' '\n",
    "            command = command+dwiInterim_dir+'/dti_raw.mif.gz'\n",
    "\n",
    "            print(command)\n",
    "            try:\n",
    "                subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "                raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Merge the topup files\n",
    "        print('\\nmerge '+str(len(topup_files))+' topup files')\n",
    "        command = 'mrcat -axis 3 '\n",
    "        for topup_file in topup_files:\n",
    "            command = command+topup_file+' '\n",
    "        command = command+dwiInterim_dir+'/dti_topup_raw.mif.gz'\n",
    "\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        ##############################\n",
    "        # Denoise the DWI data\n",
    "        print('\\n3. Denoise dwi and topup files')\n",
    "        ##############################\n",
    "        command = 'dwidenoise -nthreads 4'\n",
    "        command = command +' -noise '+dwiInterim_dir+'/dti_noisemap.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_raw.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_denoised.mif.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Creating a (noise) residual\n",
    "        command = 'mrcalc '+dwiInterim_dir+'/dti_raw.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_denoised.mif.gz'\n",
    "        command = command +' -subtract '+dwiInterim_dir+'/dti_residuals.mif.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        command = 'dwidenoise -nthreads 4'\n",
    "        command = command +' -noise '+dwiInterim_dir+'/dti_tp_noisemap.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_topup_raw.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_tp_denoised.mif.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        command = 'mrcalc '+dwiInterim_dir+'/dti_topup_raw.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_tp_denoised.mif.gz'\n",
    "        command = command +' -subtract '+dwiInterim_dir+'/dti_tp_residuals.mif.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        ##############################\n",
    "        # Degibbs the data\n",
    "        print('\\n3. Degibbs dwi and topup files')\n",
    "        ##############################\n",
    "        # DWI data\n",
    "        command = 'mrdegibbs -nthreads 4 -axes 0,1'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_denoised.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_unring.mif.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Converting the output back again to nifti output\n",
    "        command = 'mrconvert -export_grad_fsl '+dwiInterim_dir+'/dti_unring.bvec'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_unring.bval'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_unring.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_unring.nii.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Topup data\n",
    "        command = 'mrdegibbs -nthreads 4 -axes 0,1'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_tp_denoised.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_tp_unring.mif.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Converting the output back again to nifti output\n",
    "        command = 'mrconvert '+dwiInterim_dir+'/dti_tp_unring.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dti_tp_unring.nii.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        ##############################\n",
    "        # Run topup on the data\n",
    "        print('\\n3. Run topup')\n",
    "        ##############################\n",
    "        #. Note that this takes hours.\n",
    "\n",
    "        # Get the total readout time from the json file\n",
    "        # Find the json files that we still have in the main folder\n",
    "        dwiInfoJsonFiles = glob.glob(convert_dir+'/*b0_b*.json' , recursive=False)\n",
    "        topupInfoJsonFiles = glob.glob(convert_dir+'/*_phase_*.json' , recursive=False)\n",
    "        tmpJson = open(dwiInfoJsonFiles[0])\n",
    "        JsonData = json.load(tmpJson)\n",
    "        dwiTotalReadoutTime = JsonData['TotalReadoutTime']\n",
    "\n",
    "        tmpJson = open(topupInfoJsonFiles[0])\n",
    "        JsonData = json.load(tmpJson)\n",
    "        topupTotalReadoutTime = JsonData['TotalReadoutTime']\n",
    "\n",
    "        # top-up requires an even number of slices: pad if necessary\n",
    "        # DWI data\n",
    "        img = nib.load(dwiInterim_dir+'/dti_unring.nii.gz')\n",
    "        data = img.get_fdata()\n",
    "\n",
    "        if (data.shape[2]%2==1):\n",
    "            padded = np.zeros((data.shape[0],data.shape[1],data.shape[2]+1,data.shape[3]))\n",
    "            padded[0:data.shape[0],0:data.shape[1],0:data.shape[2],0:data.shape[3]] = data\n",
    "        else:\n",
    "            padded = data\n",
    "\n",
    "        img = nib.Nifti1Image(padded, affine=img.affine,header=img.header)\n",
    "        nib.save(img, dwiInterim_dir+'/dti.nii.gz')\n",
    "\n",
    "        shutil.copy(dwiInterim_dir+'/dti_unring.bval', dwiInterim_dir+'/dti.bval')\n",
    "        shutil.copy(dwiInterim_dir+'/dti_unring.bvec', dwiInterim_dir+'/dti.bvec')\n",
    "\n",
    "        # Topup data\n",
    "        img = nib.load(dwiInterim_dir+'/dti_tp_unring.nii.gz')\n",
    "        data = img.get_fdata()\n",
    "\n",
    "        if (data.shape[2]%2==1):\n",
    "            padded = np.zeros((data.shape[0],data.shape[1],data.shape[2]+1,data.shape[3]))\n",
    "            padded[0:data.shape[0],0:data.shape[1],0:data.shape[2],0:data.shape[3]] = data\n",
    "        else:\n",
    "            padded = data\n",
    "\n",
    "        img = nib.Nifti1Image(padded, affine=img.affine, header=img.header)\n",
    "        nib.save(img, dwiInterim_dir+'/dti_tp.nii.gz')\n",
    "\n",
    "        # run the fsl topup script\n",
    "        # 1. (extract total readout time from json files)\n",
    "        totreadout_main = dwiTotalReadoutTime\n",
    "        totreadout_topup = topupTotalReadoutTime\n",
    "\n",
    "        # 2. (merge all B0 from normal and topup acquisitions)\n",
    "        command = 'dwiextract -bzero'\n",
    "        command = command +' -fslgrad '+dwiInterim_dir+'/dti.bvec '+dwiInterim_dir+'/dti.bval'\n",
    "        command = command +' '+dwiInterim_dir+'/dti.nii.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/b0.nii.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        shutil.copy(dwiInterim_dir+'/dti_tp.nii.gz', dwiInterim_dir+'/b0_tp.nii.gz')\n",
    "\n",
    "        command = 'mrcat -axis 3 '\n",
    "        command = command +' '+dwiInterim_dir+'/b0.nii.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/b0_tp.nii.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/b0_all.nii.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # 3. (build the corresponding acquisition parameter file)\n",
    "        acqfile = open(dwiInterim_dir+'/acqparams.txt', 'w')\n",
    "        b0_img = nib.load(dwiInterim_dir+'/b0.nii.gz')\n",
    "        b0_tp_img = nib.load(dwiInterim_dir+'/b0_tp.nii.gz')\n",
    "        for file in range(b0_img.header.get_data_shape()[3]):\n",
    "            acqfile.write(\"0 1 0 \"+str(totreadout_main)+\"\\n\")\n",
    "        for file in range(b0_tp_img.header.get_data_shape()[3]):\n",
    "            # The first B0 tp has the same blib direction as the main DWI file\n",
    "            if file == 0:\n",
    "                acqfile.write(\"0 1 0 \"+str(totreadout_main)+\"\\n\")\n",
    "            else:\n",
    "                acqfile.write(\"0 -1 0 \"+str(topupTotalReadoutTime)+\"\\n\")\n",
    "        acqfile.close()\n",
    "\n",
    "        idxfile = open(dwiInterim_dir+'/index.txt', 'w')\n",
    "        dti_img = nib.load(dwiInterim_dir+'/dti.nii.gz')\n",
    "        for file in range(dti_img.header.get_data_shape()[3]):\n",
    "            idxfile.write(\"1  \")\n",
    "        idxfile.close()\n",
    "\n",
    "        # 4. (run topup)\n",
    "        # Note that we are using a custom b02b0_hires config file where we \n",
    "        # increased the resolution and updated the subsample steps:\n",
    "        # warpres: 2.5; subsample: 1; smoothing: 0; miter: 25\n",
    "        command = 'topup --imain='+dwiInterim_dir+'/b0_all.nii.gz'\n",
    "        command = command +' --datain='+dwiInterim_dir+'/acqparams.txt'\n",
    "        command = command +' --config='+syntaxDir+'/b02b0_hires.cnf'\n",
    "        command = command +' --out='+dwiInterim_dir+'/topup'\n",
    "        command = command +' --fout='+dwiInterim_dir+'/tfield'\n",
    "        command = command +' --iout='+dwiInterim_dir+'/topuped'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # 5. (average A and P DWIs)\n",
    "        command = 'fslmaths '+dwiInterim_dir+'/topuped -Tmean '+dwiInterim_dir+'/meanB0_AP'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # 6. (mask with BET)\n",
    "        command = 'bet '+dwiInterim_dir+'/meanB0_AP '+dwiInterim_dir+'/bet_meanB0_AP -f 0.1 -m -R'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # 7. (eddy)\n",
    "        #. Note that if you can get eddy_openmp to work on your setup \n",
    "        #. you can start to use multiple cores which is highly recommended\n",
    "        command = 'eddy_openmp --imain='+dwiInterim_dir+'/dti.nii.gz'\n",
    "        command = command +' --mask='+dwiInterim_dir+'/bet_meanB0_AP_mask.nii.gz'\n",
    "        command = command +' --acqp='+dwiInterim_dir+'/acqparams.txt'\n",
    "        command = command +' --index='+dwiInterim_dir+'/index.txt'\n",
    "        command = command +' --bvecs='+dwiInterim_dir+'/dti.bvec'\n",
    "        command = command +' --bvals='+dwiInterim_dir+'/dti.bval'\n",
    "        command = command +' --topup='+dwiInterim_dir+'/topup'\n",
    "        command = command +' --repol'\n",
    "        command = command +' --out='+dwiInterim_dir+'/eddy_corr_data'\n",
    "        command = command +' --nvoxhp=4000 --ol_nstd=3 --slm=linear --mb=1 --fwhm=10,0,0,0,0'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        ##############################\n",
    "        # N4 bias field correction\n",
    "        print('\\nBias field correction')\n",
    "        ##############################\n",
    "        # Combine the eddy corrected data into a mif file\n",
    "        command = 'mrconvert -fslgrad '+dwiInterim_dir+'/dti.bvec'\n",
    "        command = command +' '+dwiInterim_dir+'/dti.bval'\n",
    "        command = command +' '+dwiInterim_dir+'/eddy_corr_data.nii.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dwi-preproc.mif.gz'\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Run bias correction\n",
    "        command = 'dwibiascorrect ants '+dwiInterim_dir+'/dwi-preproc.mif.gz'\n",
    "        command = command +' '+dwiInterim_dir+'/dwi-preproc_unbiased.mif.gz'\n",
    "        command = command +' -bias '+dwiInterim_dir+'/bias.mif'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Create a mask of the bias corrected data\n",
    "        command = 'dwi2mask '+dwiInterim_dir+'/dwi-preproc_unbiased.mif.gz'\n",
    "        command = command+' '+dwiInterim_dir+'/mask.mif' \n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Fill the internal holes of the mask\n",
    "        # First convert it to nifti\n",
    "        command = 'mrconvert '+dwiInterim_dir+'/mask.mif '+dwiInterim_dir+'/mask.nii.gz' \n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # then fill holes\n",
    "        command = 'fslmaths '+dwiInterim_dir+'/mask.nii.gz'\n",
    "        command = command+' -bin -fillh '+dwiInterim_dir+'/mask_filled.nii.gz'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "        # Convert back to mif\n",
    "        command = 'mrconvert '+dwiInterim_dir+'/mask_filled.nii.gz '+dwiInterim_dir+'/mask_filled.mif.gz' \n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "        ##############################\n",
    "        # Fit the actual tensor model to go from dwi to dti\n",
    "        print('Fit tensor model')\n",
    "        ##############################\n",
    "        # Fit a tensor model to the DWI data where we also try to predict the signal based on the fitted model\n",
    "        #. This will be used to calculate the raw residual\n",
    "        command = 'dwi2tensor '+dwiInterim_dir+'/dwi-preproc_unbiased.mif.gz '+dwiInterim_dir+'/dti-tensor.mif.gz -predicted_signal '+dwiInterim_dir+'/dwi_predict.mif'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # Calculate the residual of the measured signal versus the predicted signal\n",
    "        command = 'mrcalc '+dwiInterim_dir+'/dwi-preproc_unbiased.mif.gz '+dwiInterim_dir+'/dwi_predict.mif -subtract '+dwiInterim_dir+'/residual.nii.gz'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        # In the end we want to have the following tensor metrices:\n",
    "        #. ADC, FA, RD, AD and the main vector maps\n",
    "        command = 'tensor2metric -fa '+dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-fa.nii.gz -rd '+dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-rd.nii.gz -ad '+dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-ad.nii.gz -adc '+dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-adc.nii.gz -vec '+dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-vec.nii.gz -mask '+dwiInterim_dir+'/mask_filled.nii.gz '+dwiInterim_dir+'/dti-tensor.mif.gz'\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "            \n",
    "        # Clean up\n",
    "        #. If save intermediate is false we want to remove the entire folder:\n",
    "        if saveIntermediatedFiles == False:\n",
    "            shutil.rmtree(dwiInterim_dir, ignore_errors=True)\n",
    "            \n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the ASL scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Process the ASL data\n",
    "#################################################################################\n",
    "# Note that betting these scans is super hard.. So we are ignoring those for now and  \n",
    "#. are just moving them to the right folder. \n",
    "overwrite = True\n",
    "\n",
    "# Copy the ASL data to the right folder and try to bet it\n",
    "for i in range(0, len(subjects)):\n",
    "    print(subjects[i])\n",
    "    if session == 'pilot':\n",
    "        work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "    else:\n",
    "        work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "        \n",
    "    convert_dir = work_dir+'/dcm2nii'\n",
    "    scansOfNoInterest_dir = convert_dir+'/scansOfNoInterest'\n",
    "    anat_dir = work_dir+'/anat'\n",
    "    perf_dir = work_dir+'/perf'\n",
    "    \n",
    "    if ((os.path.exists(perf_dir)) & (overwrite == False)):\n",
    "        print('Process the ASL data: done (set overwrite to true to recompute)')\n",
    "    else:\n",
    "        print('\\n Process the ASL data')\n",
    "        try: \n",
    "            shutil.rmtree(perf_dir, ignore_errors=True)\n",
    "            os.makedirs(perf_dir)\n",
    "        except:\n",
    "            os.makedirs(perf_dir)\n",
    "        # Get the ASL processed data\n",
    "        #. Note that this is data which is processed manually at the console based\n",
    "        #. on the raw ASL data. We are not going to process the raw ASL data here\n",
    "        #. Instead we are just going to copy the processed data and give it the right\n",
    "        #. BIDS names and folder location:\n",
    "        processedASLData = glob.glob(scansOfNoInterest_dir+'/*cbf*.*' , recursive=False)\n",
    "        print(processedASLData)\n",
    "        for file in processedASLData:\n",
    "            print(file)\n",
    "            shutil.copy(file, perf_dir)\n",
    "\n",
    "        # rename the nifti file:\n",
    "        old_name = glob.glob(perf_dir+'/*cbf*.nii.gz', recursive=False)[0]  \n",
    "        new_name =  perf_dir+'/'+subjects[i]+'_'+session+'_asl_desc-cbf.nii.gz'\n",
    "        os.rename(old_name, new_name)\n",
    "\n",
    "        # rename the json file:\n",
    "        old_nameJson = glob.glob(perf_dir+'/*cbf*.json', recursive=False)[0]  \n",
    "        new_nameJson =  perf_dir+'/'+subjects[i]+'_'+session+'_asl_desc-cbf.json'\n",
    "        os.rename(old_nameJson, new_nameJson)\n",
    "         \n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Register everything to the MP2RAGE and MNI space\n",
    "#################################################################################\n",
    "# The scans that we want to register to the MP2RAGE space are \n",
    "#. the QSM image\n",
    "#. the mean B0 image\n",
    "#. The scans that we want to register to MNI space are:\n",
    "#. the mp2rage image\n",
    "#. the QSM image\n",
    "#. the mean B0 image\n",
    "overwrite = True\n",
    "\n",
    "for i in range(0, len(subjects)):\n",
    "    print(subjects[i])\n",
    "\n",
    "    #. BIDS'ish layout: subject, session, modality, data\n",
    "    #. Where we make a seperate folder for pilot data and final data in the ISLAND folder\n",
    "    if session == 'pilot':\n",
    "        work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "    else:\n",
    "        work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "    \n",
    "    # Local path settings\n",
    "    anat_dir = work_dir+'/anat'\n",
    "    dwi_dir = work_dir+'/dwi'\n",
    "    perf_dir = work_dir+'/perf'\n",
    "    mp2rageSpace_dir = work_dir+'/anat_space'\n",
    "    mniSpace_dir = work_dir+'/mni_space'\n",
    "    \n",
    "    if ((os.path.exists(mp2rageSpace_dir)) & (overwrite == False)):\n",
    "        print('Register to mp2rage and MNI space data: done (set overwrite to true to recompute)')\n",
    "    else:\n",
    "        print('\\n Register to mp2rage and MNI space')\n",
    "        try: \n",
    "            shutil.rmtree(mp2rageSpace_dir, ignore_errors=True)\n",
    "            os.makedirs(mp2rageSpace_dir)\n",
    "            shutil.rmtree(mniSpace_dir, ignore_errors=True)\n",
    "            os.makedirs(mniSpace_dir)\n",
    "        except:\n",
    "            os.makedirs(mp2rageSpace_dir)\n",
    "            os.makedirs(mniSpace_dir)\n",
    "        \n",
    "        # Location of the MNI template (is defined at the top)\n",
    "        mniTemplate = mniTemplate\n",
    "        \n",
    "        # Location of the scans:\n",
    "        mp2rage = anat_dir+'/'+subjects[i]+'_'+session+'_T1w.nii.gz'\n",
    "        mp2rageT1map = anat_dir+'/'+subjects[i]+'_'+session+'_T1map.nii.gz'\n",
    "        mp2rageT1map2MNI = mniSpace_dir+'/'+subjects[i]+'_'+session+'_T1map_2_mni.nii.gz'\n",
    "        \n",
    "        # QSM \n",
    "        #. Instead of using the calculated QSM image we are going to use the \n",
    "        #. magnitude image for the registration as it contains more anatomical info\n",
    "        #. and then apply the transformation to the actual QSM image. \n",
    "        qsm4registration = work_dir+'/bet/'+subjects[i]+'_'+session+'_brain_Chimap.nii.gz'\n",
    "        qsm = anat_dir+'/'+subjects[i]+'_'+session+'_Chimap.nii.gz'\n",
    "        qsm2mp2rage = mp2rageSpace_dir+'/'+subjects[i]+'_'+session+'_Chimap_2_mp2rage.nii.gz'\n",
    "        qsm2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_Chimap_2_mni.nii.gz'\n",
    "        \n",
    "        # DWI.\n",
    "        #. Similarly for the DTI based metrices we are going to use the mean B0 \n",
    "        #. image for the registration and then apply the transformation to the \n",
    "        #. FA, AD etc\n",
    "        dwi4registration = dwi_dir+'/intermediateFiles/bet_meanB0_AP.nii.gz'\n",
    "        dwi2mp2rage = mp2rageSpace_dir+'/'+subjects[i]+'_'+session+'_meanB0_2_mp2rage.nii.gz'\n",
    "        dwi2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_meanB0_2_mni.nii.gz'\n",
    "    \n",
    "        # The different DTI based metrices and the corresponding output names:\n",
    "        dtiVec = dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-vec.nii.gz'\n",
    "        dtiFA = dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-fa.nii.gz'\n",
    "        dtiRD = dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-rd.nii.gz'\n",
    "        dtiAD = dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-ad.nii.gz'\n",
    "        dtiADC = dwi_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-adc.nii.gz'\n",
    "\n",
    "        dtiVec2mp2rage = mp2rageSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-vec_2_mp2rage.nii.gz'\n",
    "        dtiFA2mp2rage = mp2rageSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-fa_2_mp2rage.nii.gz'\n",
    "        dtiRD2mp2rage = mp2rageSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-rd_2_mp2rage.nii.gz'\n",
    "        dtiAD2mp2rage = mp2rageSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-ad_2_mp2rage.nii.gz'\n",
    "        dtiADC2mp2rage = mp2rageSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-adc_2_mp2rage.nii.gz'\n",
    "\n",
    "        dtiVec2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-vec_2_mni.nii.gz'\n",
    "        dtiFA2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-fa_2_mni.nii.gz'\n",
    "        dtiRD2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-rd_2_mni.nii.gz'\n",
    "        dtiAD2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-ad_2_mni.nii.gz'\n",
    "        dtiADC2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-adc_2_mni.nii.gz'\n",
    "\n",
    "        # The QSM and DWI inputs have already been betted, we just need to apply the brain mask of the MP2RAGE INV2 to\n",
    "        #. the T1w image:\n",
    "        mp2rageMask = work_dir+'/bet/'+subjects[i]+'_'+session+'_brain_T1w-INV2_mask.nii.gz'\n",
    "        mp2rageBet = anat_dir+'/'+subjects[i]+'_'+session+'_T1w_bet.nii.gz'\n",
    "        \n",
    "        # Mask the mp2rage image with the brain mask\n",
    "        command = 'fslmaths '+mp2rage+' -mas '+mp2rageMask+' '+mp2rageBet\n",
    "\n",
    "        print(command)\n",
    "        try:\n",
    "            subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "            raise subprocess.CalledProcessError(msg)\n",
    "\n",
    "        \n",
    "        # 0) Estimate deformation field image (moving) -> MNI (fixed) using ANTs\n",
    "        # Start registration\n",
    "        registrationStepsMNI = [1, 2]\n",
    "        for registrationStep in registrationStepsMNI:\n",
    "            if registrationStep == 1:\n",
    "                fixed = ants.image_read(mniTemplate)\n",
    "                moving = ants.image_read(mp2rageBet)\n",
    "                #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.75)\n",
    "\n",
    "                mytx0 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'SyN' , write_composite_transform = True)\n",
    "                warped_moving = mytx0['warpedmovout']\n",
    "                #fixed.plot(overlay = warped_moving, title = 'After First Registration', overlay_alpha = 0.75)\n",
    "\n",
    "            if registrationStep == 2:\n",
    "                fixed = ants.image_read(mniTemplate)\n",
    "                moving = warped_moving #ants.image_read(warped_moving)\n",
    "                #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.75)\n",
    "\n",
    "                mytx1 = ants.registration(fixed = fixed , moving = moving, type_of_transform = 'SyNAggro', write_composite_transform = True)\n",
    "                warped_moving = mytx1['warpedmovout']\n",
    "                #fixed.plot(overlay = warped_moving, title = 'After Second Registration Optimization', overlay_alpha = 0.75)\n",
    "\n",
    "        # Check if the direct registration worked\n",
    "        fixed = ants.image_read(mniTemplate)\n",
    "        moving = ants.image_read(mp2rageBet)\n",
    "        fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.75)\n",
    "        warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx0['fwdtransforms'], mytx1['fwdtransforms']])\n",
    "        fixed.plot(overlay = warpedimage, title = 'After Direct mp2rage 2 mni Registration', overlay_alpha = 0.75)\n",
    "        \n",
    "        # And apply to the qMRI T1 map\n",
    "        fixed = ants.image_read(mniTemplate)\n",
    "        moving = ants.image_read(mp2rageT1map)\n",
    "        movingName =  mp2rageT1map2MNI\n",
    "        warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx0['fwdtransforms'], mytx1['fwdtransforms']])\n",
    "\n",
    "        fixedNibabel = ants.to_nibabel(fixed)\n",
    "        warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "        # Where I need to copy the affine matrix of the B0 image because the warpfield does something weird with the matrix\n",
    "        #. which then interfers with mrview\n",
    "        image2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "        image2Nifti.to_filename(movingName)\n",
    "        \n",
    "    # 1) Estimate deformation field image (moving) -> MP2RAGE (fixed) using ANTs\n",
    "        #. After some testing QSM seems to work as long as you stay away from severe \n",
    "        #. nonlinear methods (probably would gain improvement by making the mp2rage betting\n",
    "        #. even better). DWI works well with dense Rigid and then followed by a single nonlinear step\n",
    "        modalities2mp2rage = ['qsm', 'dwi']\n",
    "        for modality in modalities2mp2rage:\n",
    "            print(modality)\n",
    "            if modality == 'qsm':\n",
    "                movingImageReg = qsm4registration\n",
    "                movingImage= qsm\n",
    "                outputImageMP2RAGE = qsm2mp2rage\n",
    "                outputImageMNI = qsm2mni\n",
    "                additionalMovingImages = []\n",
    "                typeOfTransform1 = 'Affine'\n",
    "                metric = 'GC'\n",
    "            elif modality == 'dwi':\n",
    "                movingImageReg = dwi4registration\n",
    "                movingImage = dwi4registration\n",
    "                outputImageMP2RAGE = dwi2mp2rage\n",
    "                outputImageMNI = dwi2mni\n",
    "                additionalMovingImages = [dtiVec, dtiFA, dtiRD, dtiAD, dtiADC]\n",
    "                additionalMovingImagesNamesMP2RAGE = [dtiVec2mp2rage, dtiFA2mp2rage, dtiRD2mp2rage, dtiAD2mp2rage, dtiADC2mp2rage]\n",
    "                additionalMovingImagesNamesMNI = [dtiVec2mni, dtiFA2mni, dtiRD2mni, dtiAD2mni, dtiADC2mni]\n",
    "                typeOfTransform1 = 'Affine'\n",
    "                metric = 'mattes'\n",
    "\n",
    "            # Start registration\n",
    "            #. within subject, single step registration seemed to work better. \n",
    "            registrationStepsMP2RAGE = [1] \n",
    "            for registrationStep in registrationStepsMP2RAGE:\n",
    "                if registrationStep == 1:\n",
    "                    fixed = ants.image_read(mp2rageBet)\n",
    "                    moving = ants.image_read(movingImageReg)\n",
    "                    #fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.75)\n",
    "\n",
    "                    mytx2 = ants.registration(fixed = fixed , moving = moving, type_of_transform = typeOfTransform1, aff_metric = metric, write_composite_transform = True)\n",
    "                    warped_moving = mytx1['warpedmovout']\n",
    "                    #fixed.plot(overlay = warped_moving, title = 'After First Registration', overlay_alpha = 0.75)\n",
    "\n",
    "            # Check if the direct registration 2 mp2rage worked\n",
    "            fixed = ants.image_read(mp2rageBet)\n",
    "            moving = ants.image_read(movingImageReg)\n",
    "            fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.75)\n",
    "            warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx2['fwdtransforms']])\n",
    "            fixed.plot(overlay = warpedimage, title = 'After Direct Registration MP2RAGE', overlay_alpha = 0.75)\n",
    "            \n",
    "            # 3) Save the ants image to nifti\n",
    "            # Export the ANTSimage to a nifti via nibabel\n",
    "            fixedNibabel = ants.to_nibabel(fixed)\n",
    "            warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "            # Where I need to copy the affine matrix of the B0 image because the warpfield does something weird with the matrix\n",
    "            #. which then interfers with mrview\n",
    "            movingimage2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "            movingimage2Nifti.to_filename(outputImageMP2RAGE)\n",
    "            \n",
    "            # Check if the direct registration 2 mni worked\n",
    "            fixed = ants.image_read(mniTemplate)\n",
    "            moving = ants.image_read(movingImage)\n",
    "            fixed.plot(overlay = moving, title = 'Before Registration', overlay_alpha = 0.75)\n",
    "            warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx2['fwdtransforms'], mytx0['fwdtransforms'], mytx1['fwdtransforms'] ]) \n",
    "            fixed.plot(overlay = warpedimage, title = 'After Direct Registration MNI', overlay_alpha = 0.75)\n",
    "            \n",
    "            # 3) Save the ants image to nifti\n",
    "            # Export the ANTSimage to a nifti via nibabel\n",
    "            fixedNibabel = ants.to_nibabel(fixed)\n",
    "            warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "            # Where I need to copy the affine matrix of the fixed image to the moving image because the warpfield does something weird with the matrix\n",
    "            #. which then interfers with mrview\n",
    "            movingimage2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "            movingimage2Nifti.to_filename(outputImageMNI)\n",
    "          \n",
    "            # Apply the transformation matrix to the different images that still need to be \n",
    "            #. transformed to mp2rage Space\n",
    "            for ii in range(0, len(additionalMovingImages)):\n",
    "                fixed = ants.image_read(mp2rageBet)\n",
    "                moving = ants.image_read(additionalMovingImages[ii])\n",
    "                movingName = additionalMovingImagesNamesMP2RAGE[ii]\n",
    "                try:\n",
    "                    warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx2['fwdtransforms']])\n",
    "                except:\n",
    "                    # Some DTI images are 4D and thus you need to change the imagetype\n",
    "                    warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx2['fwdtransforms']], imagetype = 3)\n",
    "                fixedNibabel = ants.to_nibabel(fixed)\n",
    "                warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "                # Where I need to copy the affine matrix of the fixed image to the moving image because the warpfield does something weird with the matrix\n",
    "                #. which then interfers with mrview\n",
    "                image2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "                image2Nifti.to_filename(movingName)\n",
    "                \n",
    "            #. Any image that need to be transformed to MP2RAGE also needs to go to MNI Space\n",
    "            for ii in range(0, len(additionalMovingImages)):\n",
    "                fixed = ants.image_read(mniTemplate)\n",
    "                moving = ants.image_read(additionalMovingImages[ii])\n",
    "                movingName = additionalMovingImagesNamesMNI[ii]\n",
    "                try:\n",
    "                    warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx0['fwdtransforms'], mytx1['fwdtransforms'], mytx2['fwdtransforms']])\n",
    "                except:\n",
    "                    # Some DTI images are 4D and thus you need to change the imagetype\n",
    "                    warpedimage = ants.apply_transforms(fixed=fixed, moving=moving, transformlist=[mytx0['fwdtransforms'], mytx1['fwdtransforms'], mytx2['fwdtransforms']], imagetype = 3)\n",
    "                fixedNibabel = ants.to_nibabel(fixed)\n",
    "                warpedimageNibabel = ants.to_nibabel(warpedimage)\n",
    "                # Where I need to copy the affine matrix of the B0 image because the warpfield does something weird with the matrix\n",
    "                #. which then interfers with mrview\n",
    "                image2Nifti = warpedimageNibabel.__class__(warpedimageNibabel.dataobj[:], fixedNibabel.affine, warpedimageNibabel.header)\n",
    "                image2Nifti.to_filename(movingName)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Extract qMRI metrics from the different scans in MNI space\n",
    "#################################################################################\n",
    "# We have a number of qMRI metrices in MNI space and we now want to know what the \n",
    "#. mean and std values are of a number of regions. The regions are based on an \n",
    "#. MNI atlas: http://www.brainnetome.org/\n",
    "#. Note that the MNI template used for this atlas is not identical in dimensions compared\n",
    "#. to the MNI template used in the pipeline. So we have to copy the dimensions to\n",
    "#. the atlas file. \n",
    "#. This was done with the flirt:\n",
    "#.   flirt -in BN_Atlas_246_1mm.nii -ref avg152T1_brain.nii.gz -out BN_Atlas_246_1mm_fsl.nii.gz \n",
    "#.      -bins 256 -cost corratio -searchrx 0 0 -searchry 0 0 -searchrz 0 0 -dof 6 \n",
    "#.      -schedule /usr/local/fsl/etc/flirtsch/sch3Dtrans_3dof  -interp nearestneighbour\n",
    "\n",
    "\n",
    "# Finding the atlases, templates etc\n",
    "fslImageStats = fsl.ImageStats() \n",
    "mniAtlasLabels = mniAtlasLabels # defined at the beginning\n",
    "mniAtlas = mniAtlas # defined at the beginning\n",
    "mniAtlasHeader = syntaxDir+'/MNIAtlas/BN_Atlas_246_1mm_fsl.nii'\n",
    "dfmniAtlasLabels = pd.read_csv(mniAtlasLabels, header = None, sep = ' ') # Read in the labels \n",
    "dfmniAtlasLabels.columns = ['Index', 'Structure', 'r', 'g', 'b', 'notUsed'] # and make the labels nice\n",
    "\n",
    "# Create seperate masks per structure based on the MNI atlas\n",
    "print('Splitting masks')\n",
    "for i in range(0, 247):\n",
    "    structure = dfmniAtlasLabels['Structure'].iloc[i]\n",
    "    structure = structure.replace(r'/', '-')\n",
    "    masksLabel = syntaxDir+'/MNIAtlas/'+str(structure)+'.nii.gz'\n",
    "    command = 'fslmaths '+mniAtlasHeader+' -thr '+str(i)+' -uthr '+str(i)+' '+masksLabel\n",
    "    try:\n",
    "        subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        msg = 'execution failed (error code '+str(e.returncode)+')\\n Output: '+str(e.output)\n",
    "        raise subprocess.CalledProcessError(msg)\n",
    "        \n",
    "# Get all the seperate masks as a list\n",
    "MNIMasks = syntaxDir+'/MNIAtlas'\n",
    "listOfStructures = glob.glob(MNIMasks+'/*.nii.gz')\n",
    "\n",
    "# For all subjects, contrasts and structures we want to have the mean and std value in an excel file    \n",
    "print('Extracting qMRI values')\n",
    "qMRIValueExtraction = []\n",
    "for i in range(0, len(subjects)):\n",
    "    print(subjects[i])\n",
    "    \n",
    "    # Local path settings\n",
    "    if session == 'pilot':\n",
    "        work_dir = processedDataDir+'/pilot/'+subjects[i]+'/'+session\n",
    "    else:\n",
    "        work_dir = processedDataDir+'/ISLAND/'+subjects[i]+'/'+session\n",
    "    anat_dir = work_dir+'/anat'\n",
    "    dwi_dir = work_dir+'/dwi'\n",
    "    perf_dir = work_dir+'/perf'\n",
    "    mp2rageSpace_dir = work_dir+'/anat_space'\n",
    "    mniSpace_dir = work_dir+'/mni_space'\n",
    "\n",
    "    # Which files in MNI space do we actually want to extract the values from?\n",
    "    mp2rageT1map2MNI = mniSpace_dir+'/'+subjects[i]+'_'+session+'_T1map_2_mni.nii.gz'\n",
    "    qsm2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_Chimap_2_mni.nii.gz'    \n",
    "    dtiVec2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-vec_2_mni.nii.gz'\n",
    "    dtiFA2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-fa_2_mni.nii.gz'\n",
    "    dtiRD2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-rd_2_mni.nii.gz'\n",
    "    dtiAD2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-ad_2_mni.nii.gz'\n",
    "    dtiADC2mni = mniSpace_dir+'/'+subjects[i]+'_'+session+'_dwi_desc-adc_2_mni.nii.gz'\n",
    "    # Lets make a list of those files and provide names that we are going to use in the excel sheet\n",
    "    contrastsImage = [mp2rageT1map2MNI, qsm2mni, dtiVec2mni, dtiFA2mni, dtiRD2mni, dtiAD2mni, dtiADC2mni]\n",
    "    contrastNames = ['T1', 'QSM', 'DTI-Vec', 'DTI-FA', 'DTI-RD', 'DTI-AD', 'DTI-ADC']\n",
    "    # For all the contrast images and all anatomical structures included in the atlas extract\n",
    "    #. the mean and std values and put them in a list:\n",
    "    for j in range(0, len(contrastsImage)):\n",
    "        print(contrastsImage[j])\n",
    "        for structure in listOfStructures:\n",
    "            structureName = structure.split('/')[-1].split('.')[0]\n",
    "            valueExtract = fslImageStats.run(in_file = contrastsImage[j],  mask_file = structure, op_string = '-k %s -m -s').outputs.out_stat\n",
    "            qMRIValueExtraction.append({'Subject': subjects[i], 'Contrast':contrastNames[j], 'Structure': structureName, 'Mean': valueExtract[0], 'Std':valueExtract[1]})\n",
    "\n",
    "# Export the qMRI values to a dataframe and then to an excel file:\n",
    "dfContrastValues = pd.DataFrame(qMRIValueExtraction)\n",
    "dfContrastValues.to_excel(processedDataDir+'/ISLAND_BNAtlas_qMRI-values.xlsx')\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
